# -*- coding: utf-8 -*-
"""Optuna-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13_oeridU_3tlRAbrJ5cbtck8ObwnBwor
"""

pip install optuna

pip install lightgbm

"""# 2 Inch Small Break LOCA"""

import numpy as np
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
import lightgbm as lgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load and Preprocess the Data
def load_data():
    data = pd.read_csv("/work/clustered_dataset11.csv")
    X = data.drop(columns=['Fault'])  # Features
    y = data['Fault']  # Labels (10 classes: 0-9)
    return X, y

X, y = load_data()

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 2: LightGBM for Feature Selection
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train_scaled, y_train)

# Get top 10 features
important_features = np.argsort(lgb_model.feature_importances_)[::-1][:10]
X_train_selected = X_train_scaled[:, important_features]
X_test_selected = X_test_scaled[:, important_features]

# Reshape for BRNN (3D)
X_train_reshaped = X_train_selected.reshape((X_train_selected.shape[0], 1, X_train_selected.shape[1]))
X_test_reshaped = X_test_selected.reshape((X_test_selected.shape[0], 1, X_test_selected.shape[1]))

# Step 3: Hyperparameter Optimization with Optuna
def objective(trial):
    lstm_units = trial.suggest_int('lstm_units', 32, 128)
    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])

    # Define the model
    model = Sequential([
        Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
        Bidirectional(LSTM(lstm_units, return_sequences=False)),
        Dropout(dropout_rate),
        Dense(32, activation='relu'),
        Dropout(dropout_rate),
        Dense(10, activation='softmax')
    ])

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    history = model.fit(X_train_reshaped, y_train,
                        validation_data=(X_test_reshaped, y_test),
                        epochs=10, batch_size=batch_size, verbose=0)

    val_accuracy = max(history.history['val_accuracy'])  # Track best validation accuracy
    return val_accuracy

# Run the Optuna study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)

# Best hyperparameters
best_params = study.best_params
print("Best Parameters:", best_params)

# Step 4: Train the Final Model Using Best Parameters
best_model = Sequential([
    Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
    Bidirectional(LSTM(32, return_sequences=False)),  # Reduce LSTM units
    Dropout(0.2),  # Reduce dropout
    Dense(16, activation='relu'),  # Reduce Dense layer size
    Dropout(0.2),
    Dense(10, activation='softmax')  # Output layer
])

best_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),
                   loss='sparse_categorical_crossentropy',
                   metrics=['accuracy'])

history = best_model.fit(X_train_reshaped, y_train,
                         validation_data=(X_test_reshaped, y_test),
                         epochs=20, batch_size=best_params['batch_size'], verbose=1)

# Step 5: Evaluate the Model
evaluation = best_model.evaluate(X_test_reshaped, y_test, verbose=1)
print(f"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}")

# Step 6: Confusion Matrix
y_pred = np.argmax(best_model.predict(X_test_reshaped), axis=1)
cm = confusion_matrix(y_test, y_pred, labels=range(5))


# Accuracy, Precision, Recall
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"Precision: {precision}")
print(f"Recall: {recall}")

# Plot colorful confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', xticklabels=['Normal', 'Bias', 'Flat \nLine', 'Intermittent \nSignals', 'Drift'],
            yticklabels=['Normal', 'Bias', 'Flat \nLine', 'Intermittent \nSignals', 'Drift'])
plt.title('2 inch SBLOCA Fault Detection' )
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Get feature importance from LightGBM
feature_importance = lgb_model.feature_importances_

# Sort features by importance (descending order)
important_features = np.argsort(feature_importance)[::-1]

# Select top 10 features
top_features = important_features[:10]

import time

# Select a single test sample
X_sample = X_test_scaled[:1]  # Take one sample

# Measure total inference time
start_time = time.time()

# Step 1: LightGBM Feature Selection
X_sample_selected = X_sample[:, top_features]  # Select top features
X_sample_selected = X_sample_selected.reshape((1, 1, X_sample_selected.shape[1]))  # Reshape for BRNN

# Step 2: BRNN Prediction
brnn_prediction = best_model.predict(X_sample_selected, verbose=0)

end_time = time.time()

print(f"Total Inference Time (LightGBM + BRNN): {(end_time - start_time) * 1000:.4f} ms")

"""# 6 Inch Small Break LOCA"""

import numpy as np
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
import lightgbm as lgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load and Preprocess the Data
def load_data():
    data = pd.read_csv("/work/clustered_dataset12.csv")
    X = data.drop(columns=['Fault'])  # Features
    y = data['Fault']  # Labels (10 classes: 0-9)
    return X, y

X, y = load_data()

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 2: LightGBM for Feature Selection
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train_scaled, y_train)

# Get top 10 features
important_features = np.argsort(lgb_model.feature_importances_)[::-1][:10]
X_train_selected = X_train_scaled[:, important_features]
X_test_selected = X_test_scaled[:, important_features]

# Reshape for BRNN (3D)
X_train_reshaped = X_train_selected.reshape((X_train_selected.shape[0], 1, X_train_selected.shape[1]))
X_test_reshaped = X_test_selected.reshape((X_test_selected.shape[0], 1, X_test_selected.shape[1]))

# Step 3: Hyperparameter Optimization with Optuna
def objective(trial):
    lstm_units = trial.suggest_int('lstm_units', 32, 128)
    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])

    # Define the model
    model = Sequential([
        Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
        Bidirectional(LSTM(lstm_units, return_sequences=False)),
        Dropout(dropout_rate),
        Dense(32, activation='relu'),
        Dropout(dropout_rate),
        Dense(10, activation='softmax')
    ])

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    history = model.fit(X_train_reshaped, y_train,
                        validation_data=(X_test_reshaped, y_test),
                        epochs=10, batch_size=batch_size, verbose=0)

    val_accuracy = max(history.history['val_accuracy'])  # Track best validation accuracy
    return val_accuracy

# Run the Optuna study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)

# Best hyperparameters
best_params = study.best_params
print("Best Parameters:", best_params)

# Step 4: Train the Final Model Using Best Parameters
best_model = Sequential([
    Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
    Bidirectional(LSTM(32, return_sequences=False)),  # Reduce LSTM units
    Dropout(0.2),  # Reduce dropout
    Dense(16, activation='relu'),  # Reduce Dense layer size
    Dropout(0.2),
    Dense(10, activation='softmax')  # Output layer
])

best_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),
                   loss='sparse_categorical_crossentropy',
                   metrics=['accuracy'])

history = best_model.fit(X_train_reshaped, y_train,
                         validation_data=(X_test_reshaped, y_test),
                         epochs=20, batch_size=best_params['batch_size'], verbose=1)

# Step 5: Evaluate the Model
evaluation = best_model.evaluate(X_test_reshaped, y_test, verbose=1)
print(f"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}")

# Step 6: Confusion Matrix
y_pred = np.argmax(best_model.predict(X_test_reshaped), axis=1)
cm = confusion_matrix(y_test, y_pred, labels=range(5))

# Accuracy, Precision, Recall
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"Precision: {precision}")
print(f"Recall: {recall}")

# Plot colorful confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', xticklabels=['Normal', 'Bias', 'Flat \nLine', 'Intermittent \nSignals', 'Drift'],
            yticklabels=['Normal', 'Bias', 'Flat \nLine', 'Intermittent \nSignals', 'Drift'])
plt.title('6 inch SBLOCA Fault Detection' )
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Get feature importance from LightGBM
feature_importance = lgb_model.feature_importances_

# Sort features by importance (descending order)
important_features = np.argsort(feature_importance)[::-1]

# Select top 10 features
top_features = important_features[:10]

import time

# Select a single test sample
X_sample = X_test_scaled[:1]  # Take one sample

# Measure total inference time
start_time = time.time()

# Step 1: LightGBM Feature Selection
X_sample_selected = X_sample[:, top_features]  # Select top features
X_sample_selected = X_sample_selected.reshape((1, 1, X_sample_selected.shape[1]))  # Reshape for BRNN

# Step 2: BRNN Prediction
brnn_prediction = best_model.predict(X_sample_selected, verbose=0)

end_time = time.time()

print(f"Total Inference Time (LightGBM + BRNN): {(end_time - start_time) * 1000:.4f} ms")

"""# 10 Inch Small Break LOCA"""

import numpy as np
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
import lightgbm as lgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load and Preprocess the Data
def load_data():
    data = pd.read_csv("/work/clustered_dataset13.csv")
    X = data.drop(columns=['Fault'])  # Features
    y = data['Fault']  # Labels (10 classes: 0-9)
    return X, y

X, y = load_data()

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 2: LightGBM for Feature Selection
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train_scaled, y_train)

# Get top 10 features
important_features = np.argsort(lgb_model.feature_importances_)[::-1][:10]
X_train_selected = X_train_scaled[:, important_features]
X_test_selected = X_test_scaled[:, important_features]

# Reshape for BRNN (3D)
X_train_reshaped = X_train_selected.reshape((X_train_selected.shape[0], 1, X_train_selected.shape[1]))
X_test_reshaped = X_test_selected.reshape((X_test_selected.shape[0], 1, X_test_selected.shape[1]))

# Step 3: Hyperparameter Optimization with Optuna
def objective(trial):
    lstm_units = trial.suggest_int('lstm_units', 32, 128)
    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])

    # Define the model
    model = Sequential([
        Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
        Bidirectional(LSTM(lstm_units, return_sequences=False)),
        Dropout(dropout_rate),
        Dense(32, activation='relu'),
        Dropout(dropout_rate),
        Dense(10, activation='softmax')
    ])

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    history = model.fit(X_train_reshaped, y_train,
                        validation_data=(X_test_reshaped, y_test),
                        epochs=10, batch_size=batch_size, verbose=0)

    val_accuracy = max(history.history['val_accuracy'])  # Track best validation accuracy
    return val_accuracy

# Run the Optuna study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)

# Best hyperparameters
best_params = study.best_params
print("Best Parameters:", best_params)

# Step 4: Train the Final Model Using Best Parameters
best_model = Sequential([
    Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
    Bidirectional(LSTM(32, return_sequences=False)),  # Reduce LSTM units
    Dropout(0.2),  # Reduce dropout
    Dense(16, activation='relu'),  # Reduce Dense layer size
    Dropout(0.2),
    Dense(10, activation='softmax')  # Output layer
])

best_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),
                   loss='sparse_categorical_crossentropy',
                   metrics=['accuracy'])

history = best_model.fit(X_train_reshaped, y_train,
                         validation_data=(X_test_reshaped, y_test),
                         epochs=20, batch_size=best_params['batch_size'], verbose=1)

# Step 5: Evaluate the Model
evaluation = best_model.evaluate(X_test_reshaped, y_test, verbose=1)
print(f"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}")

# Step 6: Confusion Matrix
y_pred = np.argmax(best_model.predict(X_test_reshaped), axis=1)
cm = confusion_matrix(y_test, y_pred, labels=range(5))

# Accuracy, Precision, Recall
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"Precision: {precision}")
print(f"Recall: {recall}")

# Plot colorful confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', xticklabels=['Normal', 'Bias', 'Flat \nLine', 'Intermittent \nSignals', 'Drift'],
            yticklabels=['Normal', 'Bias', 'Flat \nLine', 'Intermittent \nSignals', 'Drift'])
plt.title('10 inch SBLOCA Fault Detection' )
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Get feature importance from LightGBM
feature_importance = lgb_model.feature_importances_

# Sort features by importance (descending order)
important_features = np.argsort(feature_importance)[::-1]

# Select top 10 features
top_features = important_features[:10]

import time

# Select a single test sample
X_sample = X_test_scaled[:1]  # Take one sample

# Measure total inference time
start_time = time.time()

# Step 1: LightGBM Feature Selection
X_sample_selected = X_sample[:, top_features]  # Select top features
X_sample_selected = X_sample_selected.reshape((1, 1, X_sample_selected.shape[1]))  # Reshape for BRNN

# Step 2: BRNN Prediction
brnn_prediction = best_model.predict(X_sample_selected, verbose=0)

end_time = time.time()

print(f"Total Inference Time (LightGBM + BRNN): {(end_time - start_time) * 1000:.4f} ms")

"""# 50% Inch Large Break LOCA"""

import numpy as np
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
import lightgbm as lgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load and Preprocess the Data
def load_data():
    data = pd.read_csv("/work/clustered_dataset14.csv")
    X = data.drop(columns=['Fault'])  # Features
    y = data['Fault']  # Labels (10 classes: 0-9)
    return X, y

X, y = load_data()

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 2: LightGBM for Feature Selection
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train_scaled, y_train)

# Get top 10 features
important_features = np.argsort(lgb_model.feature_importances_)[::-1][:10]
X_train_selected = X_train_scaled[:, important_features]
X_test_selected = X_test_scaled[:, important_features]

# Reshape for BRNN (3D)
X_train_reshaped = X_train_selected.reshape((X_train_selected.shape[0], 1, X_train_selected.shape[1]))
X_test_reshaped = X_test_selected.reshape((X_test_selected.shape[0], 1, X_test_selected.shape[1]))

# Step 3: Hyperparameter Optimization with Optuna
def objective(trial):
    lstm_units = trial.suggest_int('lstm_units', 32, 128)
    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])

    # Define the model
    model = Sequential([
        Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
        Bidirectional(LSTM(lstm_units, return_sequences=False)),
        Dropout(dropout_rate),
        Dense(32, activation='relu'),
        Dropout(dropout_rate),
        Dense(10, activation='softmax')
    ])

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    history = model.fit(X_train_reshaped, y_train,
                        validation_data=(X_test_reshaped, y_test),
                        epochs=10, batch_size=batch_size, verbose=0)

    val_accuracy = max(history.history['val_accuracy'])  # Track best validation accuracy
    return val_accuracy

# Run the Optuna study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)

# Best hyperparameters
best_params = study.best_params
print("Best Parameters:", best_params)

# Step 4: Train the Final Model Using Best Parameters
best_model = Sequential([
    Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
    Bidirectional(LSTM(32, return_sequences=False)),  # Reduce LSTM units
    Dropout(0.2),  # Reduce dropout
    Dense(16, activation='relu'),  # Reduce Dense layer size
    Dropout(0.2),
    Dense(10, activation='softmax')  # Output layer
])

best_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),
                   loss='sparse_categorical_crossentropy',
                   metrics=['accuracy'])

history = best_model.fit(X_train_reshaped, y_train,
                         validation_data=(X_test_reshaped, y_test),
                         epochs=20, batch_size=best_params['batch_size'], verbose=1)

# Step 5: Evaluate the Model
evaluation = best_model.evaluate(X_test_reshaped, y_test, verbose=1)
print(f"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}")

# Step 6: Confusion Matrix
y_pred = np.argmax(best_model.predict(X_test_reshaped), axis=1)
cm = confusion_matrix(y_test, y_pred, labels=range(5))

# Accuracy, Precision, Recall
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"Precision: {precision}")
print(f"Recall: {recall}")

# Plot colorful confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', xticklabels=['Normal', 'Clipping', 'Out-of-Range \nReadings', 'Sudden High or \nLow Spikes \nand Noise', 'Delayed \nResponse'],
            yticklabels=['Normal', 'Clipping', 'Out-of-Range \nReadings', 'Sudden High or \nLow Spikes \nand Noise', 'Delayed \nResponse'])
plt.title('50% LBLOCA Fault Detection' )
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Get feature importance from LightGBM
feature_importance = lgb_model.feature_importances_

# Sort features by importance (descending order)
important_features = np.argsort(feature_importance)[::-1]

# Select top 10 features
top_features = important_features[:10]

import time

# Select a single test sample
X_sample = X_test_scaled[:1]  # Take one sample

# Measure total inference time
start_time = time.time()

# Step 1: LightGBM Feature Selection
X_sample_selected = X_sample[:, top_features]  # Select top features
X_sample_selected = X_sample_selected.reshape((1, 1, X_sample_selected.shape[1]))  # Reshape for BRNN

# Step 2: BRNN Prediction
brnn_prediction = best_model.predict(X_sample_selected, verbose=0)

end_time = time.time()

print(f"Total Inference Time (LightGBM + BRNN): {(end_time - start_time) * 1000:.4f} ms")

"""# 75% Inch Large Break LOCA"""

import numpy as np
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
import lightgbm as lgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load and Preprocess the Data
def load_data():
    data = pd.read_csv("/work/clustered_dataset15.csv")
    X = data.drop(columns=['Fault'])  # Features
    y = data['Fault']  # Labels (10 classes: 0-9)
    return X, y

X, y = load_data()

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 2: LightGBM for Feature Selection
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train_scaled, y_train)

# Get top 10 features
important_features = np.argsort(lgb_model.feature_importances_)[::-1][:10]
X_train_selected = X_train_scaled[:, important_features]
X_test_selected = X_test_scaled[:, important_features]

# Reshape for BRNN (3D)
X_train_reshaped = X_train_selected.reshape((X_train_selected.shape[0], 1, X_train_selected.shape[1]))
X_test_reshaped = X_test_selected.reshape((X_test_selected.shape[0], 1, X_test_selected.shape[1]))

# Step 3: Hyperparameter Optimization with Optuna
def objective(trial):
    lstm_units = trial.suggest_int('lstm_units', 32, 128)
    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])

    # Define the model
    model = Sequential([
        Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
        Bidirectional(LSTM(lstm_units, return_sequences=False)),
        Dropout(dropout_rate),
        Dense(32, activation='relu'),
        Dropout(dropout_rate),
        Dense(10, activation='softmax')
    ])

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    history = model.fit(X_train_reshaped, y_train,
                        validation_data=(X_test_reshaped, y_test),
                        epochs=10, batch_size=batch_size, verbose=0)

    val_accuracy = max(history.history['val_accuracy'])  # Track best validation accuracy
    return val_accuracy

# Run the Optuna study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)

# Best hyperparameters
best_params = study.best_params
print("Best Parameters:", best_params)

# Step 4: Train the Final Model Using Best Parameters
best_model = Sequential([
    Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),
    Bidirectional(LSTM(32, return_sequences=False)),  # Reduce LSTM units
    Dropout(0.2),  # Reduce dropout
    Dense(16, activation='relu'),  # Reduce Dense layer size
    Dropout(0.2),
    Dense(10, activation='softmax')  # Output layer
])

best_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),
                   loss='sparse_categorical_crossentropy',
                   metrics=['accuracy'])

history = best_model.fit(X_train_reshaped, y_train,
                         validation_data=(X_test_reshaped, y_test),
                         epochs=20, batch_size=best_params['batch_size'], verbose=1)

# Step 5: Evaluate the Model
evaluation = best_model.evaluate(X_test_reshaped, y_test, verbose=1)
print(f"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}")

# Step 6: Confusion Matrix
y_pred = np.argmax(best_model.predict(X_test_reshaped), axis=1)
cm = confusion_matrix(y_test, y_pred, labels=range(5))

# Accuracy, Precision, Recall
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"Precision: {precision}")
print(f"Recall: {recall}")

# Plot colorful confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', xticklabels=['Normal', 'Clipping', 'Out-of-Range \nReadings', 'Sudden High or \nLow Spikes \nand Noise', 'Delayed \nResponse'],
            yticklabels=['Normal', 'Clipping', 'Out-of-Range \nReadings', 'Sudden High or \nLow Spikes \nand Noise', 'Delayed \nResponse'])
plt.title('75% LBLOCA Fault Detection' )
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Get feature importance from LightGBM
feature_importance = lgb_model.feature_importances_

# Sort features by importance (descending order)
important_features = np.argsort(feature_importance)[::-1]

# Select top 10 features
top_features = important_features[:10]

import time

# Select a single test sample
X_sample = X_test_scaled[:1]  # Take one sample

# Measure total inference time
start_time = time.time()

# Step 1: LightGBM Feature Selection
X_sample_selected = X_sample[:, top_features]  # Select top features
X_sample_selected = X_sample_selected.reshape((1, 1, X_sample_selected.shape[1]))  # Reshape for BRNN

# Step 2: BRNN Prediction
brnn_prediction = best_model.predict(X_sample_selected, verbose=0)

end_time = time.time()

print(f"Total Inference Time (LightGBM + BRNN): {(end_time - start_time) * 1000:.4f} ms")

"""<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0b0f6b38-519a-4bc0-b8da-28f7daea01b4' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>
"""