# -*- coding: utf-8 -*-
"""Baseline Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e3K6ymW21qtwjHRbNNwZoDnADhrjHJur

# 2-inch SBLOCA
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.optimizers import Adam

# Load and preprocess the data
data = pd.read_csv("/work/clustered_dataset11.csv")  # Replace with your dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with your target column name
y = data['Fault']  # Replace 'target_column' with your target column name

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    'Random Forest': RandomForestClassifier(),
    'Voting Ensemble': VotingClassifier(estimators=[
        ('rf', RandomForestClassifier()),
        ('svc', SVC(kernel='linear'))
    ], voting='hard'),
    'AdaBoost': AdaBoostClassifier(),
    'Bagging': BaggingClassifier(),
    'SVM': SVC(kernel='linear'),
    'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),
}

# Train and evaluate all models (except LSTM) and print accuracy
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {accuracy * 100:.2f}%")

# LSTM model (Keras)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

lstm_model = Sequential([
    LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

lstm_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Train and evaluate LSTM model
lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
lstm_accuracy = lstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"LSTM Accuracy: {lstm_accuracy * 100:.2f}%")

"""# 6-inch SBLOCA"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.optimizers import Adam

# Load and preprocess the data
data = pd.read_csv("/work/clustered_dataset12.csv")  # Replace with your dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with your target column name
y = data['Fault']  # Replace 'target_column' with your target column name

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    'Random Forest': RandomForestClassifier(),
    'Voting Ensemble': VotingClassifier(estimators=[
        ('rf', RandomForestClassifier()),
        ('svc', SVC(kernel='linear'))
    ], voting='hard'),
    'AdaBoost': AdaBoostClassifier(),
    'Bagging': BaggingClassifier(),
    'SVM': SVC(kernel='linear'),
    'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),
}

# Train and evaluate all models (except LSTM) and print accuracy
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {accuracy * 100:.2f}%")

# LSTM model (Keras)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

lstm_model = Sequential([
    LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

lstm_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Train and evaluate LSTM model
lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
lstm_accuracy = lstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"LSTM Accuracy: {lstm_accuracy * 100:.2f}%")

"""# 10-inch SBLOCA"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.optimizers import Adam

# Load and preprocess the data
data = pd.read_csv("/work/clustered_dataset13.csv")  # Replace with your dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with your target column name
y = data['Fault']  # Replace 'target_column' with your target column name

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    'Random Forest': RandomForestClassifier(),
    'Voting Ensemble': VotingClassifier(estimators=[
        ('rf', RandomForestClassifier()),
        ('svc', SVC(kernel='linear'))
    ], voting='hard'),
    'AdaBoost': AdaBoostClassifier(),
    'Bagging': BaggingClassifier(),
    'SVM': SVC(kernel='linear'),
    'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),
}

# Train and evaluate all models (except LSTM) and print accuracy
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {accuracy * 100:.2f}%")

# LSTM model (Keras)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

lstm_model = Sequential([
    LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

lstm_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Train and evaluate LSTM model
lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
lstm_accuracy = lstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"LSTM Accuracy: {lstm_accuracy * 100:.2f}%")

"""# 50% LBLOCA"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.optimizers import Adam

# Load and preprocess the data
data = pd.read_csv("/work/clustered_dataset14.csv")  # Replace with your dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with your target column name
y = data['Fault']  # Replace 'target_column' with your target column name

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    'Random Forest': RandomForestClassifier(),
    'Voting Ensemble': VotingClassifier(estimators=[
        ('rf', RandomForestClassifier()),
        ('svc', SVC(kernel='linear'))
    ], voting='hard'),
    'AdaBoost': AdaBoostClassifier(),
    'Bagging': BaggingClassifier(),
    'SVM': SVC(kernel='linear'),
    'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),
}

# Train and evaluate all models (except LSTM) and print accuracy
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {accuracy * 100:.2f}%")

# LSTM model (Keras)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

lstm_model = Sequential([
    LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

lstm_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Train and evaluate LSTM model
lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
lstm_accuracy = lstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"LSTM Accuracy: {lstm_accuracy * 100:.2f}%")

"""# 75% LBLOCA"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.optimizers import Adam

# Load and preprocess the data
data = pd.read_csv("/work/clustered_dataset15.csv")  # Replace with your dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with your target column name
y = data['Fault']  # Replace 'target_column' with your target column name

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    'Random Forest': RandomForestClassifier(),
    'Voting Ensemble': VotingClassifier(estimators=[
        ('rf', RandomForestClassifier()),
        ('svc', SVC(kernel='linear'))
    ], voting='hard'),
    'AdaBoost': AdaBoostClassifier(),
    'Bagging': BaggingClassifier(),
    'SVM': SVC(kernel='linear'),
    'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),
}

# Train and evaluate all models (except LSTM) and print accuracy
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {accuracy * 100:.2f}%")

# LSTM model (Keras)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

lstm_model = Sequential([
    LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

lstm_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Train and evaluate LSTM model
lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
lstm_accuracy = lstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"LSTM Accuracy: {lstm_accuracy * 100:.2f}%")

"""# GBC

# 2-inch
"""

pip install xgboost

import numpy as np
import pandas as pd
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, GRU, Dense, Dropout, LSTM
from tensorflow.keras.optimizers import Adam

# Load and preprocess the data
data = pd.read_csv("/work/clustered_dataset11.csv")  # Replace with your dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with your target column name
y = data['Fault']  # Replace 'target_column' with your target column name

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define and train XGBoost model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_accuracy = accuracy_score(y_test, xgb_pred)
print(f"XGBoost Accuracy: {xgb_accuracy * 100:.2f}%")

# Define and train LightGBM model
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train_scaled, y_train)
lgb_pred = lgb_model.predict(X_test_scaled)
lgb_accuracy = accuracy_score(y_test, lgb_pred)
print(f"LightGBM Accuracy: {lgb_accuracy * 100:.2f}%")

# Reshape the data for RNN and GRU models (3D)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Define and train ANN model (Artificial Neural Network)
ann_model = Sequential([
    Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' if multi-class classification
])

ann_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])  # Adjust for multi-class if needed
ann_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)

ann_accuracy = ann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
print(f"ANN Accuracy: {ann_accuracy * 100:.2f}%")

# Define and train RNN model
rnn_model = Sequential([
    SimpleRNN(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

rnn_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
rnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
rnn_accuracy = rnn_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"RNN Accuracy: {rnn_accuracy * 100:.2f}%")

# Define and train GRU model
gru_model = Sequential([
    GRU(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

gru_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
gru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
gru_accuracy = gru_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"GRU Accuracy: {gru_accuracy * 100:.2f}%")

"""# 6-inch"""

import numpy as np
import pandas as pd
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, GRU, Dense, Dropout, LSTM
from tensorflow.keras.optimizers import Adam

# Load and preprocess the data
data = pd.read_csv("/work/clustered_dataset12.csv")  # Replace with your dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with your target column name
y = data['Fault']  # Replace 'target_column' with your target column name

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define and train XGBoost model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_accuracy = accuracy_score(y_test, xgb_pred)
print(f"XGBoost Accuracy: {xgb_accuracy * 100:.2f}%")

# Define and train LightGBM model
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train_scaled, y_train)
lgb_pred = lgb_model.predict(X_test_scaled)
lgb_accuracy = accuracy_score(y_test, lgb_pred)
print(f"LightGBM Accuracy: {lgb_accuracy * 100:.2f}%")

# Reshape the data for RNN and GRU models (3D)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Define and train RNN model
rnn_model = Sequential([
    SimpleRNN(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])
# Define and train ANN model (Artificial Neural Network)
ann_model = Sequential([
    Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' if multi-class classification
])

ann_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])  # Adjust for multi-class if needed
ann_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)

ann_accuracy = ann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
print(f"ANN Accuracy: {ann_accuracy * 100:.2f}%")

rnn_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
rnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
rnn_accuracy = rnn_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"RNN Accuracy: {rnn_accuracy * 100:.2f}%")

# Define and train GRU model
gru_model = Sequential([
    GRU(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

gru_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
gru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
gru_accuracy = gru_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"GRU Accuracy: {gru_accuracy * 100:.2f}%")

"""# 10-inch"""

import numpy as np
import pandas as pd
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, GRU, Dense, Dropout, LSTM
from tensorflow.keras.optimizers import Adam

# Load and preprocess the data
data = pd.read_csv("/work/clustered_dataset13.csv")  # Replace with your dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with your target column name
y = data['Fault']  # Replace 'target_column' with your target column name

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define and train XGBoost model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_accuracy = accuracy_score(y_test, xgb_pred)
print(f"XGBoost Accuracy: {xgb_accuracy * 100:.2f}%")

# Define and train LightGBM model
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train_scaled, y_train)
lgb_pred = lgb_model.predict(X_test_scaled)
lgb_accuracy = accuracy_score(y_test, lgb_pred)
print(f"LightGBM Accuracy: {lgb_accuracy * 100:.2f}%")

# Reshape the data for RNN and GRU models (3D)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))
# Define and train ANN model (Artificial Neural Network)
ann_model = Sequential([
    Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' if multi-class classification
])

ann_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])  # Adjust for multi-class if needed
ann_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)

ann_accuracy = ann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
print(f"ANN Accuracy: {ann_accuracy * 100:.2f}%")
# Define and train RNN model
rnn_model = Sequential([
    SimpleRNN(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

rnn_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
rnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
rnn_accuracy = rnn_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"RNN Accuracy: {rnn_accuracy * 100:.2f}%")

# Define and train GRU model
gru_model = Sequential([
    GRU(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

gru_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
gru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
gru_accuracy = gru_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"GRU Accuracy: {gru_accuracy * 100:.2f}%")

"""# 50%"""

import numpy as np
import pandas as pd
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, GRU, Dense, Dropout, LSTM
from tensorflow.keras.optimizers import Adam

# Load and preprocess the data
data = pd.read_csv("/work/clustered_dataset14.csv")  # Replace with your dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with your target column name
y = data['Fault']  # Replace 'target_column' with your target column name

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define and train XGBoost model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_accuracy = accuracy_score(y_test, xgb_pred)
print(f"XGBoost Accuracy: {xgb_accuracy * 100:.2f}%")

# Define and train LightGBM model
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train_scaled, y_train)
lgb_pred = lgb_model.predict(X_test_scaled)
lgb_accuracy = accuracy_score(y_test, lgb_pred)
print(f"LightGBM Accuracy: {lgb_accuracy * 100:.2f}%")

# Reshape the data for RNN and GRU models (3D)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))
# Define and train ANN model (Artificial Neural Network)
ann_model = Sequential([
    Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' if multi-class classification
])

ann_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])  # Adjust for multi-class if needed
ann_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)

ann_accuracy = ann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
print(f"ANN Accuracy: {ann_accuracy * 100:.2f}%")
# Define and train RNN model
rnn_model = Sequential([
    SimpleRNN(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

rnn_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
rnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
rnn_accuracy = rnn_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"RNN Accuracy: {rnn_accuracy * 100:.2f}%")

# Define and train GRU model
gru_model = Sequential([
    GRU(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

gru_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
gru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
gru_accuracy = gru_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"GRU Accuracy: {gru_accuracy * 100:.2f}%")

"""# 75%"""

import numpy as np
import pandas as pd
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, GRU, Dense, Dropout, LSTM
from tensorflow.keras.optimizers import Adam

# Load and preprocess the data
data = pd.read_csv("/work/clustered_dataset15.csv")  # Replace with your dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with your target column name
y = data['Fault']  # Replace 'target_column' with your target column name

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define and train XGBoost model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_accuracy = accuracy_score(y_test, xgb_pred)
print(f"XGBoost Accuracy: {xgb_accuracy * 100:.2f}%")

# Define and train LightGBM model
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train_scaled, y_train)
lgb_pred = lgb_model.predict(X_test_scaled)
lgb_accuracy = accuracy_score(y_test, lgb_pred)
print(f"LightGBM Accuracy: {lgb_accuracy * 100:.2f}%")

# Reshape the data for RNN and GRU models (3D)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))
# Define and train ANN model (Artificial Neural Network)
ann_model = Sequential([
    Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' if multi-class classification
])

ann_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])  # Adjust for multi-class if needed
ann_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)

ann_accuracy = ann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
print(f"ANN Accuracy: {ann_accuracy * 100:.2f}%")
# Define and train RNN model
rnn_model = Sequential([
    SimpleRNN(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

rnn_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
rnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
rnn_accuracy = rnn_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"RNN Accuracy: {rnn_accuracy * 100:.2f}%")

# Define and train GRU model
gru_model = Sequential([
    GRU(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Adjust according to the number of classes
])

gru_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
gru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)
gru_accuracy = gru_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"GRU Accuracy: {gru_accuracy * 100:.2f}%")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, GRU

# Load and preprocess the dataset
data = pd.read_csv("/work/clustered_dataset11.csv")  # Replace with the actual dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with the actual target column
y = data['Fault']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for CNN input (samples, timesteps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the MLH-CNN model
mlh_cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)),
    MaxPooling1D(pool_size=2),

    Conv1D(filters=128, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),

    LSTM(64, return_sequences=True),  # Hybrid LSTM Layer
    GRU(32, return_sequences=False),  # Hybrid GRU Layer

    Flatten(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
mlh_cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the MLH-CNN model
mlh_cnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
mlh_cnn_accuracy = mlh_cnn_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"MLH-CNN Accuracy: {mlh_cnn_accuracy * 100:.2f}%")

"""# 2-inch"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout

# Load and preprocess the dataset
data = pd.read_csv("/work/clustered_dataset11.csv")  # Replace with actual dataset path
X = data.drop(columns=['Fault'])  # Replace with actual target column
y = data['Fault']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for RNN input (samples, timesteps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the BiLSTM model
bilstm_model = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], 1)),
    Dropout(0.3),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the BiLSTM model
bilstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
bilstm_accuracy = bilstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"BiLSTM Accuracy: {bilstm_accuracy * 100:.2f}%")

from tensorflow.keras.layers import GRU, Bidirectional

# Define the BiGRU model
bigru_model = Sequential([
    Bidirectional(GRU(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], 1)),
    Dropout(0.3),
    Bidirectional(GRU(32)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
bigru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the BiGRU model
bigru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
bigru_accuracy = bigru_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"BiGRU Accuracy: {bigru_accuracy * 100:.2f}%")


from tensorflow.keras.layers import Flatten

# Define the FFANN model
ffann_model = Sequential([
    Flatten(input_shape=(X_train_scaled.shape[1],)),  # Flatten input features
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
ffann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the FFANN model
ffann_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
ffann_accuracy = ffann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
print(f"FFANN Accuracy: {ffann_accuracy * 100:.2f}%")

"""# 6-inch"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout

# Load and preprocess the dataset
data = pd.read_csv("/work/clustered_dataset12.csv")  # Replace with actual dataset path
X = data.drop(columns=['Fault'])  # Replace with actual target column
y = data['Fault']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for RNN input (samples, timesteps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the BiLSTM model
bilstm_model = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], 1)),
    Dropout(0.3),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the BiLSTM model
bilstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
bilstm_accuracy = bilstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"BiLSTM Accuracy: {bilstm_accuracy * 100:.2f}%")

from tensorflow.keras.layers import GRU, Bidirectional

# Define the BiGRU model
bigru_model = Sequential([
    Bidirectional(GRU(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], 1)),
    Dropout(0.3),
    Bidirectional(GRU(32)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
bigru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the BiGRU model
bigru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
bigru_accuracy = bigru_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"BiGRU Accuracy: {bigru_accuracy * 100:.2f}%")


from tensorflow.keras.layers import Flatten

# Define the FFANN model
ffann_model = Sequential([
    Flatten(input_shape=(X_train_scaled.shape[1],)),  # Flatten input features
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
ffann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the FFANN model
ffann_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
ffann_accuracy = ffann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
print(f"FFANN Accuracy: {ffann_accuracy * 100:.2f}%")



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, GRU

# Load and preprocess the dataset
data = pd.read_csv("/work/clustered_dataset12.csv")  # Replace with the actual dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with the actual target column
y = data['Fault']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for CNN input (samples, timesteps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the MLH-CNN model
mlh_cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)),
    MaxPooling1D(pool_size=2),

    Conv1D(filters=128, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),

    LSTM(64, return_sequences=True),  # Hybrid LSTM Layer
    GRU(32, return_sequences=False),  # Hybrid GRU Layer

    Flatten(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
mlh_cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the MLH-CNN model
mlh_cnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
mlh_cnn_accuracy = mlh_cnn_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"MLH-CNN Accuracy: {mlh_cnn_accuracy * 100:.2f}%")

"""# 10-inch"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout

# Load and preprocess the dataset
data = pd.read_csv("/work/clustered_dataset13.csv")  # Replace with actual dataset path
X = data.drop(columns=['Fault'])  # Replace with actual target column
y = data['Fault']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for RNN input (samples, timesteps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the BiLSTM model
bilstm_model = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], 1)),
    Dropout(0.3),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the BiLSTM model
bilstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
bilstm_accuracy = bilstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"BiLSTM Accuracy: {bilstm_accuracy * 100:.2f}%")

from tensorflow.keras.layers import GRU, Bidirectional

# Define the BiGRU model
bigru_model = Sequential([
    Bidirectional(GRU(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], 1)),
    Dropout(0.3),
    Bidirectional(GRU(32)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
bigru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the BiGRU model
bigru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
bigru_accuracy = bigru_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"BiGRU Accuracy: {bigru_accuracy * 100:.2f}%")


from tensorflow.keras.layers import Flatten

# Define the FFANN model
ffann_model = Sequential([
    Flatten(input_shape=(X_train_scaled.shape[1],)),  # Flatten input features
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
ffann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the FFANN model
ffann_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
ffann_accuracy = ffann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
print(f"FFANN Accuracy: {ffann_accuracy * 100:.2f}%")



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, GRU

# Load and preprocess the dataset
data = pd.read_csv("/work/clustered_dataset13.csv")  # Replace with the actual dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with the actual target column
y = data['Fault']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for CNN input (samples, timesteps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the MLH-CNN model
mlh_cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)),
    MaxPooling1D(pool_size=2),

    Conv1D(filters=128, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),

    LSTM(64, return_sequences=True),  # Hybrid LSTM Layer
    GRU(32, return_sequences=False),  # Hybrid GRU Layer

    Flatten(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
mlh_cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the MLH-CNN model
mlh_cnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
mlh_cnn_accuracy = mlh_cnn_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"MLH-CNN Accuracy: {mlh_cnn_accuracy * 100:.2f}%")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout

# Load and preprocess the dataset
data = pd.read_csv("/work/clustered_dataset14.csv")  # Replace with actual dataset path
X = data.drop(columns=['Fault'])  # Replace with actual target column
y = data['Fault']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for RNN input (samples, timesteps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the BiLSTM model
bilstm_model = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], 1)),
    Dropout(0.3),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the BiLSTM model
bilstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
bilstm_accuracy = bilstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"BiLSTM Accuracy: {bilstm_accuracy * 100:.2f}%")

from tensorflow.keras.layers import GRU, Bidirectional

# Define the BiGRU model
bigru_model = Sequential([
    Bidirectional(GRU(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], 1)),
    Dropout(0.3),
    Bidirectional(GRU(32)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
bigru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the BiGRU model
bigru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
bigru_accuracy = bigru_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"BiGRU Accuracy: {bigru_accuracy * 100:.2f}%")


from tensorflow.keras.layers import Flatten

# Define the FFANN model
ffann_model = Sequential([
    Flatten(input_shape=(X_train_scaled.shape[1],)),  # Flatten input features
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
ffann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the FFANN model
ffann_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
ffann_accuracy = ffann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
print(f"FFANN Accuracy: {ffann_accuracy * 100:.2f}%")



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, GRU

# Load and preprocess the dataset
data = pd.read_csv("/work/clustered_dataset14.csv")  # Replace with the actual dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with the actual target column
y = data['Fault']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for CNN input (samples, timesteps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the MLH-CNN model
mlh_cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)),
    MaxPooling1D(pool_size=2),

    Conv1D(filters=128, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),

    LSTM(64, return_sequences=True),  # Hybrid LSTM Layer
    GRU(32, return_sequences=False),  # Hybrid GRU Layer

    Flatten(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
mlh_cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the MLH-CNN model
mlh_cnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
mlh_cnn_accuracy = mlh_cnn_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"MLH-CNN Accuracy: {mlh_cnn_accuracy * 100:.2f}%")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout

# Load and preprocess the dataset
data = pd.read_csv("/work/clustered_dataset15.csv")  # Replace with actual dataset path
X = data.drop(columns=['Fault'])  # Replace with actual target column
y = data['Fault']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for RNN input (samples, timesteps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the BiLSTM model
bilstm_model = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], 1)),
    Dropout(0.3),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the BiLSTM model
bilstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
bilstm_accuracy = bilstm_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"BiLSTM Accuracy: {bilstm_accuracy * 100:.2f}%")

from tensorflow.keras.layers import GRU, Bidirectional

# Define the BiGRU model
bigru_model = Sequential([
    Bidirectional(GRU(64, return_sequences=True), input_shape=(X_train_reshaped.shape[1], 1)),
    Dropout(0.3),
    Bidirectional(GRU(32)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
bigru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the BiGRU model
bigru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
bigru_accuracy = bigru_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"BiGRU Accuracy: {bigru_accuracy * 100:.2f}%")


from tensorflow.keras.layers import Flatten

# Define the FFANN model
ffann_model = Sequential([
    Flatten(input_shape=(X_train_scaled.shape[1],)),  # Flatten input features
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
ffann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the FFANN model
ffann_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
ffann_accuracy = ffann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
print(f"FFANN Accuracy: {ffann_accuracy * 100:.2f}%")



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, GRU

# Load and preprocess the dataset
data = pd.read_csv("/work/clustered_dataset15.csv")  # Replace with the actual dataset path
X = data.drop(columns=['Fault'])  # Replace 'target_column' with the actual target column
y = data['Fault']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for CNN input (samples, timesteps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the MLH-CNN model
mlh_cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)),
    MaxPooling1D(pool_size=2),

    Conv1D(filters=128, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),

    LSTM(64, return_sequences=True),  # Hybrid LSTM Layer
    GRU(32, return_sequences=False),  # Hybrid GRU Layer

    Flatten(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification
])

# Compile the model
mlh_cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the MLH-CNN model
mlh_cnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)

# Evaluate the model
mlh_cnn_accuracy = mlh_cnn_model.evaluate(X_test_reshaped, y_test, verbose=0)[1]
print(f"MLH-CNN Accuracy: {mlh_cnn_accuracy * 100:.2f}%")

"""<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0b0f6b38-519a-4bc0-b8da-28f7daea01b4' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>
"""